{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need to create a session in it\n",
    "import pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Basics\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.json(\"people.json\")   # imported the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()  # it will show the output\n",
    "\n",
    "output:\n",
    "age   name\n",
    "null  Michael\n",
    "30    Andy\n",
    "19   Justin\n",
    "\n",
    "# it automatically replace null value with null keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to know the schema of dataframe\n",
    "df.printSchema()\n",
    "\n",
    "output:\n",
    "root\n",
    "---age: long(nullable=True)\n",
    "---name: string(nullable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns  # it will return list of columns\n",
    "\n",
    "output:\n",
    "[\"age\",\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "output:\n",
    "DataFrames[summary:string,age:string]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()  # it will show the statistic od data , pyspark is smart  tht it automatically \n",
    "#figure out numeric column is age   so it will use age column\n",
    "\n",
    "output:\n",
    "summary     age\n",
    "count       2\n",
    "mean\n",
    "stddev\n",
    "min \n",
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can change the datatye as well\n",
    " from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)\n",
    "    \n",
    "data=[StructField(\"age\",IntegerType(),True),\n",
    "     StructField(\"name\",StringType(),True)]\n",
    "\n",
    "\n",
    "final=StructType(fields=data)\n",
    "\n",
    "\n",
    "df=spark.read.json(\"people.json\",schema=final)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "output\n",
    "root\n",
    "---age: integer(nullable=True)\n",
    "---name: string(nullable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets grab column \n",
    "df[\"age\"]  \n",
    "\n",
    "# but this will not grab it . this wil return object\n",
    "output:\n",
    "    Column<b\"age\">\n",
    "\n",
    "    \n",
    "df.select(\"age\").show()  # this will work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)[0]\n",
    "# it will print recrd of firat row\n",
    "output:\n",
    "    Row(age=None,Name=\"Michael\")\n",
    "\n",
    " \n",
    "\n",
    "df.select([\"age\",\"Name\"]).show()   # it will print the result in row and column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create another column\n",
    "df.withcolumn(\"newage\",df[\"age\"]).show()\n",
    "output\n",
    "age   name     newage\n",
    "null  Michael  nul\n",
    "30    Andy     30\n",
    "19   Justin     19\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.withcolumn(\"ndouble_age\",df[\"age\"]*2).show()\n",
    "output\n",
    "age   name     ndouble_age\n",
    "null  Michael  nul\n",
    "30    Andy     60\n",
    "19   Justin     38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column name\n",
    "df.withColumnRenamed(\"age\",\"new_age\").show()\n",
    "\n",
    "output\n",
    "anew_age   name     \n",
    "null    Michael  \n",
    "30      Andy     \n",
    "19      Justin     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use SQL quey as well in pyspark  we just need t use register by below line code\n",
    "df.CreateOrReplaceTempView(\"People\")   # taking the file\n",
    "results=spark.sql(\"select * from  people\")\n",
    "results.show()  # output will be produced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"close<500\").show()   # lookm typical  , but i prefer sql \n",
    "# all column with leass than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"close<500\").select([\"Open\",\"Close\"]).show()\n",
    "\n",
    "# output will have two column in it\n",
    "df.filter(df[\"close\"]<500).select([\"Open\",\"Close\"]).show()\n",
    "# same output will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"close\"]<500   &  df[\"open\"]<500 ).show()   \n",
    "# using And opearion in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"close\"]<500   &  `df[\"open\"]<500 ).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"close\"]==500  ).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"close\"]==500  ).collect()    # collect will be use in real world problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggreate mean to collect data and perform mean ,sum,count etc\n",
    "\n",
    "df.groupby(\"Company\").mean().show()\n",
    "df.groupby(\"Company\").sum().show()\n",
    "df.groupby(\"Company\").max().show()\n",
    "df.groupby(\"Company\").min().show()\n",
    "df.groupby(\"Company\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can aggrqage on single column\n",
    "df.agg({\"Sales\":\"Sum\"}).show()   # it will show all sum\n",
    "\n",
    "sum(sales)\n",
    "1122\n",
    "\n",
    "\n",
    "df.agg({\"Sales\":\"Max\"}).show()   # it will show all max\n",
    "\n",
    "Max(sales)\n",
    "780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way\n",
    "group=df.groupby(\"Comapny\")\n",
    "group.agg({\"Sales\":\"Sum\"}).show()\n",
    "\n",
    "company   max(sales)\n",
    "apple     780\n",
    "google     111\n",
    "fb         222\n",
    "mst          333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will grab function \n",
    "from pyspark.sql.functons import countDistinct,avg,stddev\n",
    "\n",
    "df.select(avg(\"Sales\")).show()\n",
    "\n",
    "output\n",
    "avg(sales)\n",
    "360.33333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(avg(\"Sales\").alias(\"AVGSALES\")).show()\n",
    "\n",
    "output\n",
    "AVGSALES\n",
    "360.33333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i just want 2 last digit float value\n",
    "from pyspark.sql.functions import forrmat_number\n",
    "\n",
    "sales=df.select(avg(\"Sales\").alias(\"AVGSALES\")).show()\n",
    "\n",
    "sales.select(format_number(\"std\",2),).alias(\"new\").show()  # we have provde two time of alias otherwirse it will print previous one\n",
    "\n",
    "std\n",
    "250.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.OrderBy(\"Sales\").show()  # in ascending order will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderby(df[\"sales\"].desc()).show()  # in descending order wil be printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "\n",
    "output\n",
    "id name sales\n",
    "emp1 john  null\n",
    "emp2  null  null\n",
    "emp3  null 365.00\n",
    "emp4  cindy  456.00\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop().show()  # it will drop any row wiich conraon null value i it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can restrict our null value drop by using threhold value\n",
    "\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to dropping null value\n",
    "df.na.drop(how=\"any\").show()  # it wil drop any null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop(how=\"all\").show()  # if all the value is null  but in our dataset we dont have 3 null value in one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can specify our column name\n",
    "    df.na.drop(subset=[\"sales\"]).show()  # it will frop sales null value i it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can put value i null valaue\n",
    "df.na.fill(\"null value\").show()   # it automatically know that it si string so it didint fill integer column \n",
    "    \n",
    "output\n",
    "id name sales\n",
    "emp1 john  null\n",
    "emp2  null value  null\n",
    "emp3  null value 365.00\n",
    "emp4  cindy  456.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(0).show()   # it automatically know that it si integer value so it fill sales column\n",
    "    \n",
    "output\n",
    "id name sales\n",
    "emp1 john  0\n",
    "emp2  null  0\n",
    "emp3  null  365.00\n",
    "emp4  cindy  456.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates and Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth,hour,dayofyear,month,year,weekofyear,format_number,date_format)\n",
    "\n",
    "df.select(dayofmonth(df[\"Date\"])).show()\n",
    "\n",
    "output\n",
    "dayofmonth(Date)\n",
    "3\n",
    "4\n",
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(hour(df[\"Date\"])).show()\n",
    "\n",
    "output\n",
    "dayofmonth(Date)\n",
    "0\n",
    "0\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
